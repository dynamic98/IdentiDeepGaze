{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "from loadData import *\n",
    "from ML_util import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(RBF, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.centers = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.centers, -1, 1)\n",
    "        nn.init.constant_(self.sigma, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = (x.size(0), self.out_features, self.in_features)\n",
    "        x = x.unsqueeze(1).expand(size)\n",
    "        c = self.centers.unsqueeze(0).expand(size)\n",
    "        distances = torch.sum((x - c) ** 2, -1)\n",
    "        return torch.exp(-distances / (2 * self.sigma.unsqueeze(0) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentiGazeDataset(Dataset):\n",
    "    def __init__(self, path, dataType) -> None:\n",
    "        super().__init__()\n",
    "        self.loadSelectiveData = LoadResultData(path)\n",
    "        if dataType == 'train':\n",
    "            self.data = self.loadSelectiveData.take_df_by_session([1,2,3,4]).reset_index(drop=True)\n",
    "        elif dataType == 'test':\n",
    "            self.data = self.loadSelectiveData.take_df_by_session([5]).reset_index(drop=True)\n",
    "\n",
    "        self.rawgaze = self.loadSelectiveData.take_xy2d(self.data)\n",
    "        self.eyemovement = self.loadSelectiveData.take_EyeMovement(self.data)\n",
    "        self.fixation = self.loadSelectiveData.take_Fixation(self.data)\n",
    "        self.saccade = self.loadSelectiveData.take_Saccade(self.data)\n",
    "        self.MFCC = self.loadSelectiveData.take_MFCC(self.data)\n",
    "        self.pupil = self.loadSelectiveData.take_Pupil(self.data)\n",
    "        self.y = self.loadSelectiveData.take_y(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rawgaze = torch.FloatTensor(self.rawgaze.iloc[index]).transpose(0,1)\n",
    "        eyemovement = torch.FloatTensor(self.eyemovement.iloc[index])\n",
    "        fixation = torch.FloatTensor(self.fixation.iloc[index])\n",
    "        saccade = torch.FloatTensor(self.saccade.iloc[index])\n",
    "        mfcc = torch.FloatTensor(self.MFCC.iloc[index])\n",
    "        pupil = torch.FloatTensor(self.pupil.iloc[index])\n",
    "        label = self.y.iloc[index]\n",
    "        return {'rawgaze':rawgaze, 'eyemovement':eyemovement, 'fixation':fixation, 'saccade': saccade, \n",
    "                'mfcc': mfcc, 'pupil': pupil, 'y': label}\n",
    "\n",
    "path = 'C:\\\\Users\\\\scilab\\\\IdentiGaze\\\\data\\\\DayDifference\\\\Similar_All.csv'\n",
    "trainDataset = IdentiGazeDataset(path, 'train')\n",
    "testDataset = IdentiGazeDataset(path, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFN(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rbf_neurons):\n",
    "        super(RBFN, self).__init__()\n",
    "        self.rbf = RBF(in_features, rbf_neurons)\n",
    "        self.linear = nn.Linear(rbf_neurons, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rbf(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentiRBFNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IdentiRBFNet, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        # Define the output layer\n",
    "        self.output = nn.Linear(128, 34)\n",
    "\n",
    "        self.EyeMovementNet = nn.Sequential(\n",
    "            RBF(9, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.FixationNet = nn.Sequential(\n",
    "            RBF(10, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.SaccadeNet = nn.Sequential(\n",
    "            RBF(17, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.MFCCNet = nn.Sequential(\n",
    "            RBF(12, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.PupilNet = nn.Sequential(\n",
    "            RBF(12, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "\n",
    "    # def forward(self, rawgaze, eyemovement, fixation, saccade, mfcc, pupil):\n",
    "    def forward(self, eyemovement, fixation, saccade, mfcc, pupil):\n",
    "        # Apply the gated convolutions\n",
    "        # rawgaze = self.RawGazeNet(rawgaze)\n",
    "        # rawgaze = rawgaze.view(rawgaze.size(0), -1)\n",
    "        # rawgaze = F.relu(self.fc1(rawgaze))\n",
    "        # rawgaze = self.dropout(rawgaze)\n",
    "        # rawgaze = F.relu(self.fc2(rawgaze))\n",
    "        # rawgaze = self.output(rawgaze)\n",
    "\n",
    "        eyemovement = eyemovement.view(eyemovement.size(0), -1)\n",
    "        eyemovement = self.EyeMovementNet(eyemovement)\n",
    "        fixation = self.FixationNet(fixation)\n",
    "        saccade = self.SaccadeNet(saccade)\n",
    "        mfcc = self.MFCCNet(mfcc)\n",
    "        pupil = self.PupilNet(pupil)\n",
    "\n",
    "        # out = rawgaze + eyemovement + fixation + saccade + mfcc + pupil\n",
    "        out = eyemovement + fixation + saccade + mfcc + pupil\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentiDenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IdentiDenseNet, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        # Define the output layer\n",
    "        self.output = nn.Linear(128, 34)\n",
    "\n",
    "        self.EyeMovementNet = nn.Sequential(\n",
    "            nn.Linear(9, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.FixationNet = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.SaccadeNet = nn.Sequential(\n",
    "            nn.Linear(17, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.MFCCNet = nn.Sequential(\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.PupilNet = nn.Sequential(\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "\n",
    "    # def forward(self, rawgaze, eyemovement, fixation, saccade, mfcc, pupil):\n",
    "    def forward(self, eyemovement, fixation, saccade, mfcc, pupil):\n",
    "        # Apply the gated convolutions\n",
    "        # rawgaze = self.RawGazeNet(rawgaze)\n",
    "        # rawgaze = rawgaze.view(rawgaze.size(0), -1)\n",
    "        # rawgaze = F.relu(self.fc1(rawgaze))\n",
    "        # rawgaze = self.dropout(rawgaze)\n",
    "        # rawgaze = F.relu(self.fc2(rawgaze))\n",
    "        # rawgaze = self.output(rawgaze)\n",
    "\n",
    "        eyemovement = eyemovement.view(eyemovement.size(0), -1)\n",
    "        eyemovement = self.EyeMovementNet(eyemovement)\n",
    "        fixation = self.FixationNet(fixation)\n",
    "        saccade = self.SaccadeNet(saccade)\n",
    "        mfcc = self.MFCCNet(mfcc)\n",
    "        pupil = self.PupilNet(pupil)\n",
    "\n",
    "        # out = rawgaze + eyemovement + fixation + saccade + mfcc + pupil\n",
    "        out = eyemovement + fixation + saccade + mfcc + pupil\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=self.padding, dilation=dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        return x[:, :, :-self.padding]  # Remove the padding on the right\n",
    "\n",
    "class GatedActivationUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(GatedActivationUnit, self).__init__()\n",
    "        self.conv = CausalConv1d(in_channels, out_channels * 2, kernel_size, stride, dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        out, gate = x.chunk(2, dim=1)  # Split the tensor along the channel dimension\n",
    "        return torch.tanh(out) * torch.sigmoid(gate)  # Gated activation\n",
    "    \n",
    "\n",
    "class TotalRBFNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TotalRBFNet, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        # Define the output layer\n",
    "        self.output = nn.Linear(128, 34)\n",
    "\n",
    "\n",
    "        self.RawGazeNet = nn.Sequential(\n",
    "            # Input Size: (84,2)\n",
    "            CausalConv1d(2, 4, kernel_size=7, stride=1, dilation=2),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3),\n",
    "            CausalConv1d(4, 5, kernel_size=5, stride=1, dilation=4),\n",
    "            CausalConv1d(5, 16, kernel_size=3, stride=1, dilation=5),\n",
    "            CausalConv1d(16, 32, kernel_size=3, stride=1, dilation=16),\n",
    "            nn.AdaptiveMaxPool1d(1),  # Global Max Pooling\n",
    "        )\n",
    "\n",
    "\n",
    "        self.EyeMovementNet = nn.Sequential(\n",
    "            RBF(9, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.FixationNet = nn.Sequential(\n",
    "            RBF(10, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.SaccadeNet = nn.Sequential(\n",
    "            RBF(17, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.MFCCNet = nn.Sequential(\n",
    "            RBF(12, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "        self.PupilNet = nn.Sequential(\n",
    "            RBF(12, 64),\n",
    "            nn.Linear(64, 34)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, rawgaze, eyemovement, fixation, saccade, mfcc, pupil):\n",
    "        # Apply the gated convolutions\n",
    "        rawgaze = self.RawGazeNet(rawgaze)\n",
    "        rawgaze = rawgaze.view(rawgaze.size(0), -1)\n",
    "        rawgaze = F.relu(self.fc1(rawgaze))\n",
    "        rawgaze = self.dropout(rawgaze)\n",
    "        rawgaze = F.relu(self.fc2(rawgaze))\n",
    "        rawgaze = self.output(rawgaze)\n",
    "\n",
    "        eyemovement = eyemovement.view(eyemovement.size(0), -1)\n",
    "        eyemovement = self.EyeMovementNet(eyemovement)\n",
    "        fixation = self.FixationNet(fixation)\n",
    "        saccade = self.SaccadeNet(saccade)\n",
    "        mfcc = self.MFCCNet(mfcc)\n",
    "        pupil = self.PupilNet(pupil)\n",
    "\n",
    "        out = rawgaze + eyemovement + fixation + saccade + mfcc + pupil\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 34])\n"
     ]
    }
   ],
   "source": [
    "modelName = 'IdRBF'\n",
    "\n",
    "writer = SummaryWriter(f'model/{modelName}/runs')\n",
    "# idModel = IdentiRBFNet()\n",
    "# idModel = IdentiDenseNet()\n",
    "idModel = TotalRBFNet()\n",
    "# output = idModel(torch.randn(1,9).to(device),torch.randn(1,10).to(device),torch.randn(1,17).to(device),torch.randn(1,12).to(device),torch.randn(1,12).to(device))\n",
    "output = idModel(torch.randn(1,2,84).to(device), torch.randn(1,9).to(device),torch.randn(1,10).to(device),torch.randn(1,17).to(device),torch.randn(1,12).to(device),torch.randn(1,12).to(device))\n",
    "\n",
    "# print(len(myIdentiGaze))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(idModel.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(trainDataset)\n",
    "test_size = len(testDataset)\n",
    "train_loader = DataLoader(trainDataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testDataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scilab\\AppData\\Local\\Temp\\ipykernel_7352\\474243876.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  eyemovement = torch.FloatTensor(self.eyemovement.iloc[index])\n",
      "C:\\Users\\scilab\\AppData\\Local\\Temp\\ipykernel_7352\\474243876.py:24: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fixation = torch.FloatTensor(self.fixation.iloc[index])\n",
      "C:\\Users\\scilab\\AppData\\Local\\Temp\\ipykernel_7352\\474243876.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  saccade = torch.FloatTensor(self.saccade.iloc[index])\n",
      "C:\\Users\\scilab\\AppData\\Local\\Temp\\ipykernel_7352\\474243876.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mfcc = torch.FloatTensor(self.MFCC.iloc[index])\n",
      "C:\\Users\\scilab\\AppData\\Local\\Temp\\ipykernel_7352\\474243876.py:27: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pupil = torch.FloatTensor(self.pupil.iloc[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 15 %\n",
      "Accuracy of the network on the 10000 test images: 13 %\n",
      "Accuracy of the network on the 10000 test images: 9 %\n",
      "Accuracy of the network on the 10000 test images: 11 %\n",
      "Accuracy of the network on the 10000 test images: 12 %\n",
      "Accuracy of the network on the 10000 test images: 13 %\n",
      "Accuracy of the network on the 10000 test images: 15 %\n",
      "Accuracy of the network on the 10000 test images: 15 %\n",
      "Accuracy of the network on the 10000 test images: 17 %\n",
      "Accuracy of the network on the 10000 test images: 22 %\n",
      "Accuracy of the network on the 10000 test images: 22 %\n",
      "Accuracy of the network on the 10000 test images: 21 %\n",
      "Accuracy of the network on the 10000 test images: 24 %\n",
      "Accuracy of the network on the 10000 test images: 21 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 22 %\n",
      "Accuracy of the network on the 10000 test images: 23 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 24 %\n",
      "Accuracy of the network on the 10000 test images: 19 %\n",
      "Accuracy of the network on the 10000 test images: 24 %\n",
      "Accuracy of the network on the 10000 test images: 22 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 21 %\n",
      "Accuracy of the network on the 10000 test images: 22 %\n",
      "Accuracy of the network on the 10000 test images: 23 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 23 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 23 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 24 %\n",
      "Accuracy of the network on the 10000 test images: 22 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 23 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 32 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 33 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 32 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 32 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 26 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 33 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 34 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 30 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 32 %\n",
      "Accuracy of the network on the 10000 test images: 33 %\n",
      "Accuracy of the network on the 10000 test images: 32 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Accuracy of the network on the 10000 test images: 31 %\n",
      "Accuracy of the network on the 10000 test images: 33 %\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Accuracy of the network on the 10000 test images: 32 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 27 %\n",
      "Accuracy of the network on the 10000 test images: 29 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      7\u001b[0m     rawgaze \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawgaze\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mtype(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      8\u001b[0m     eyemovement \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meyemovement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mtype(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\scilab\\AppData\\Local\\anaconda3\\envs\\siameseNetwork\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\scilab\\AppData\\Local\\anaconda3\\envs\\siameseNetwork\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\scilab\\AppData\\Local\\anaconda3\\envs\\siameseNetwork\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\scilab\\AppData\\Local\\anaconda3\\envs\\siameseNetwork\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m, in \u001b[0;36mIdentiGazeDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 22\u001b[0m     rawgaze \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrawgaze\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m     eyemovement \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meyemovement\u001b[38;5;241m.\u001b[39miloc[index])\n\u001b[0;32m     24\u001b[0m     fixation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixation\u001b[38;5;241m.\u001b[39miloc[index])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        rawgaze = data['rawgaze'].to(device).type(dtype=torch.float32)\n",
    "        eyemovement = data['eyemovement'].to(device).type(dtype=torch.float32)\n",
    "        fixation = data['fixation'].to(device).type(dtype=torch.float32)\n",
    "        saccade = data['saccade'].to(device).type(dtype=torch.float32)\n",
    "        mfcc = data['mfcc'].to(device).type(dtype=torch.float32)\n",
    "        pupil = data['pupil'].to(device).type(dtype=torch.float32)\n",
    "        y = data['y'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs = idModel(eyemovement, fixation, saccade, mfcc, pupil)\n",
    "        outputs = idModel(rawgaze, eyemovement, fixation, saccade, mfcc, pupil)\n",
    "        loss = criterion(outputs, y)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    Accuracy = 100 * correct // total\n",
    "    writer.add_scalar('Acc/train', Accuracy, epoch+1)\n",
    "    writer.add_scalar('Loss/train', running_loss, epoch+1)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            rawgaze = data['rawgaze'].to(device).type(dtype=torch.float32)\n",
    "            eyemovement = data['eyemovement'].to(device).type(dtype=torch.float32)\n",
    "            fixation = data['fixation'].to(device).type(dtype=torch.float32)\n",
    "            saccade = data['saccade'].to(device).type(dtype=torch.float32)\n",
    "            mfcc = data['mfcc'].to(device).type(dtype=torch.float32)\n",
    "            pupil = data['pupil'].to(device).type(dtype=torch.float32)\n",
    "            y = data['y'].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = idModel(rawgaze, eyemovement, fixation, saccade, mfcc, pupil)\n",
    "            # outputs = idModel(eyemovement, fixation, saccade, mfcc, pupil)\n",
    "            test_loss = criterion(outputs, y)\n",
    "            writer.add_scalar('Loss/test', test_loss, epoch+1)\n",
    "\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "        Accuracy = 100 * correct // total\n",
    "        print(f'Accuracy of the network on the 10000 test images: {Accuracy} %')\n",
    "        writer.add_scalar('Acc/test', Accuracy, epoch+1)\n",
    "        torch.save(idModel.state_dict(), f'model/{modelName}/{epoch}_{100 * correct // total}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siameseNetwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
